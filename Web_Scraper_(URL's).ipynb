{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlFtOdDVLPl1wLcp/y51nm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeanThornton23/Coding-Projects-Learning-/blob/Web_Scraper-(URL's)/Web_Scraper_(URL's).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0SeznMVCojL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def extract_text(url):\n",
        "    # Send a request to the URL and get the HTML response\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the headers and paragraphs on the page\n",
        "    headers = soup.find_all('h1, h2, h3, h4, h5, h6')\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "     # Extract the text from the headers and paragraphs\n",
        "    text = ''\n",
        "    for header in headers:\n",
        "        text += header.text.strip() + '\\n\\n'\n",
        "    for paragraph in paragraphs:\n",
        "        text += paragraph.text.strip() + '\\n\\n'\n",
        "\n",
        "    # Format the text for readability\n",
        "    text = text.replace('\\n\\n', '\\n')\n",
        "    text = text.replace('\\n', '\\n\\n')\n",
        "    text = text.strip()\n",
        "\n",
        "    # Display the text in a readable format\n",
        "    print(text)\n",
        "\n",
        "# Test the function with a URL\n",
        "url = 'https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window'\n",
        "extract_text(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Here's a breakdown of what's happening in the code for the `extract_text` function:\n",
        "\n",
        "1. `import requests`: This line imports the `requests` library, which allows us to send HTTP requests to a URL.\n",
        "2. `from bs4 import BeautifulSoup`: This line imports the `BeautifulSoup` library, which allows us to parse HTML content.\n",
        "3. `import re`: This line imports the `re` library, which allows us to use regular expressions in our code.\n",
        "4. `def extract_text(url):`: This line defines a function called `extract_text`, which takes a URL as an argument.\n",
        "5. `response = requests.get(url)`: This line sends a GET request to the URL and stores the response in the `response` variable.\n",
        "6. `soup = BeautifulSoup(response.content, 'html.parser')`: This line parses the HTML content of the response using the `html.parser` parser. The parsed HTML content is stored in the `soup` variable.\n",
        "7. `headers = soup.find_all('h1, h2, h3, h4, h5, h6')`: This line finds all the headers on the page ( `<h1>`, `<h2>`, `<h3>`, `<h4>`, `<h5>`, and `<h6>`) and stores them in the `headers` variable.\n",
        "8. `paragraphs = soup.find_all('p')`: This line finds all the paragraphs on the page ( `<p>` tags) and stores them in the `paragraphs` variable.\n",
        "9. `text = ''`: This line initializes an empty string called `text`.\n",
        "10. `for header in headers:`: This line starts a loop that iterates over the headers found on the page.\n",
        "11. `text += header.text.strip() + '\\n\\n'`: This line adds the text content of each header to the `text` string, along with a newline character (`\\n\\n`) to separate each header from the next.\n",
        "12. `for paragraph in paragraphs:`: This line starts a loop that iterates over the paragraphs found on the page.\n",
        "13. `text += paragraph.text.strip() + '\\n\\n'`: This line adds the text content of each paragraph to the `text` string, along with a newline character (`\\n\\n`) to separate each paragraph from the next.\n",
        "14. `text = text.replace('\\n\\n', '\\n')`: This line replaces any double newline characters (`\\n\\n`) in the `text` string with a single newline character (`\\n`).\n",
        "15. `text = text.replace('\\n', '\\n\\n')`: This line replaces any single newline characters (`\\n`) in the `text` string with a double newline character (`\\n\\n`).\n",
        "16. `text = text.strip()`: This line removes any leading or trailing whitespace from the `text` string.\n",
        "17. `print(text)`: This line prints the final `text` string to the console.\n",
        "\n",
        "The code then defines a URL to test the function with, and calls the `extract_text` function with that URL."
      ],
      "metadata": {
        "id": "VR7IyArZGUz3"
      }
    }
  ]
}